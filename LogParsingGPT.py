import random
import openai
import os
import re

from tqdm import tqdm

instruction = """
You are an AI log analysis expert. You should look a log message given by user and generate python f-string template which can generate the log message. You should generate python code including variable assignment lines with semantic name (if you think variables exist) and template assignment line. The generated code should generate the user's input log. 
EXAMPLES:
{examples}
END EXAMPLES
""".strip()

examples = [
    "USER: 'Returning 500 to user'",
    "ASSISTANT: status_code = '500'\ntemplate = f'Returning {status_code} to user'\n",
    "USER: 'Listing instance in cell 949e1227'",
    "ASSISTANT: cell_id = '949e1227'\ntemplate = f'Listing instance in cell {cell_id}'\n",
    "USER: 'onReceive action: android.intent.action.SCREEN_ON'",
    "ASSISTANT: template = f'onReceive action: android.intent.action.SCREEN_ON'"
]

second_instruction = """
SAMPLES: {samples}
Second, above are sample log messages generated by the template you've generated. Determine if the template needs to be updated by determining if the template is correct and that the variable names are well-constructed to represent the log messages.
""".strip()

third_instruction = """
Third, you should update the template to make it more generic and meaningful. The template should still be able to generate the sample log messages.
""".strip()

class LogParsingGPT:
    def __init__(self, openai_api_key: str = None) -> None:
        openai.api_key = openai_api_key or os.environ.get('OPENAI_API_KEY')
        self.model = 'gpt-3.5-turbo'
        self.history = []
        self.instruction = instruction
        self.examples = examples
        self.second_instruction = second_instruction
        self.third_instruction = third_instruction

    def llm_run(self, user_prompt: str) -> str:
        response = openai.ChatCompletion.create(
            model=self.model,
            temperature=0.0,
            messages=[
                {"role": "system", "content": instruction.format(examples='\n'.join(self.examples))},
                *self.history,
                {"role": "user", "content": user_prompt}
            ]
        )
        return response['choices'][0]['message']['content']
    
    def output_parse(self,llm_output: str) -> dict:
        llm_output = llm_output.replace('ASSISTANT:\n', '')
        *variables, template = llm_output.split('\n')
        variables = [ [ entity.strip() for entity in var.split('=') ] for var in variables ]
        variables = [ var for var in variables if len(var) == 2 ]
        try:
            variables = { var[0]: eval(var[1]) for var in variables }
        except Exception as e:
            print(e)
            print(llm_output)
            exit(0)

        template = template.split('=')[1].strip()
        # remove f-string
        template = template[2:-1]
        # remove all '\' from template
        template = template.replace('\\', '')

        return {'variables': variables, 'template': template}
    
    def yes_or_no(self, llm_output: str) -> bool:
        if llm_output.lower().startswith('yes'):
            return True
        elif llm_output.lower().startswith('no'):
            return False
        else:
            raise ValueError('llm_output should start with yes or no')
        
def match_template(logs: list[str], regex_template: str):
    return [ log for log in logs if re.match(regex_template, log)]
     

def sem_to_regex(template: str, variables: list[str]) -> str:
    regex = re.escape(template)
    for var in variables:
        regex = regex.replace('\\{' + var + '\\}', '(.+)')
    return regex
        
if __name__ == '__main__':
    from data_utils import load_dataset
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default=None)
    parser.add_argument('--read', type=bool, default=False)
    args = parser.parse_args()

    if args.read:
        with open('result.json', 'r') as f:
            result = json.load(f)
        for output in result:
            print(f'Parsed {output["template"]} from {output["sample_log"]}')
            print(f'Matches: {len(output["unmatched"])} / {len(output["unmatched"]) + len(output["wrong_matches"])}')
            print('----------------------------------')
        exit(0)

    test_data = load_dataset(args.dataset)
    test_templates = test_data['template'].unique()

    confirm = input(f'You are going to parse {len(test_templates)} templates from {args.dataset}. Continue? (y/n) ')
    if confirm.lower() != 'y':
        exit(0)

    log_parser = LogParsingGPT()

    result = []

    for template in tqdm(test_templates):
        log_messages = test_data[test_data['template'] == template]
        log_messages = log_messages['log'].tolist()

        sample_log = random.choice(log_messages)

        llm_output = log_parser.llm_run(f"'{sample_log}'")
        output = log_parser.output_parse(llm_output)

        regex_template = sem_to_regex(output['template'], list(output['variables'].keys()))
        matches = match_template(log_messages, regex_template)
        output['oracle'] = template
        output['sample_log'] = sample_log
        output['accuracy'] = len(matches) / len(log_messages)
        output['unmatched'] = [ log for log in log_messages if log not in matches ]
        output['wrong_matches'] = [ log for log in matches if log not in log_messages ]
        result.append(output)
        print(f'Parsed {output["template"]} from {sample_log}')
        print(f'Matches: {len(matches)} / {len(log_messages)}')
        print('----------------------------------')
    
    # sve result to file
    import json
    with open(f'result_{args.dataset}.json', 'w') as f:
        json.dump(result, f, indent=4)